{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "# If you are using a Jupyter notebook, uncomment the following line.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Replace <Subscription Key> with your valid subscription key.\n",
    "subscription_key = \"<Subscription Key>\"\n",
    "assert subscription_key\n",
    "\n",
    "# You must use the same region in your REST call as you used to get your\n",
    "# subscription keys. For example, if you got your subscription keys from\n",
    "# westus, replace \"westcentralus\" in the URI below with \"westus\".\n",
    "#\n",
    "# Free trial subscription keys are generated in the \"westus\" region.\n",
    "# If you use a free trial subscription key, you shouldn't need to change\n",
    "# this region.\n",
    "vision_base_url = \"https://westcentralus.api.cognitive.microsoft.com/vision/v2.0/\"\n",
    "\n",
    "text_recognition_url = vision_base_url + \"recognizeText\"\n",
    "\n",
    "# Set image_url to the URL of an image that you want to analyze.\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/\" + \\\n",
    "    \"Cursive_Writing_on_Notebook_paper.jpg/800px-Cursive_Writing_on_Notebook_paper.jpg\"\n",
    "\n",
    "headers = {'Ocp-Apim-Subscription-Key': subscription_key}\n",
    "# Note: The request parameter changed for APIv2.\n",
    "# For APIv1, it is 'handwriting': 'true'.\n",
    "params  = {'mode': 'Handwritten'}\n",
    "data    = {'url': image_url}\n",
    "response = requests.post(\n",
    "    text_recognition_url, headers=headers, params=params, json=data)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Extracting handwritten text requires two API calls: One call to submit the\n",
    "# image for processing, the other to retrieve the text found in the image.\n",
    "\n",
    "# Holds the URI used to retrieve the recognized text.\n",
    "operation_url = response.headers[\"Operation-Location\"]\n",
    "\n",
    "# The recognized text isn't immediately available, so poll to wait for completion.\n",
    "analysis = {}\n",
    "poll = True\n",
    "while (poll):\n",
    "    response_final = requests.get(\n",
    "        response.headers[\"Operation-Location\"], headers=headers)\n",
    "    analysis = response_final.json()\n",
    "    time.sleep(1)\n",
    "    if (\"recognitionResult\" in analysis):\n",
    "        poll= False \n",
    "    if (\"status\" in analysis and analysis['status'] == 'Failed'):\n",
    "        poll= False\n",
    "\n",
    "polygons=[]\n",
    "if (\"recognitionResult\" in analysis):\n",
    "    # Extract the recognized text, with bounding boxes.\n",
    "    polygons = [(line[\"boundingBox\"], line[\"text\"])\n",
    "        for line in analysis[\"recognitionResult\"][\"lines\"]]\n",
    "\n",
    "# Display the image and overlay it with the extracted text.\n",
    "plt.figure(figsize=(15, 15))\n",
    "image = Image.open(BytesIO(requests.get(image_url).content))\n",
    "ax = plt.imshow(image)\n",
    "for polygon in polygons:\n",
    "    vertices = [(polygon[0][i], polygon[0][i+1])\n",
    "        for i in range(0, len(polygon[0]), 2)]\n",
    "    text     = polygon[1]\n",
    "    patch    = Polygon(vertices, closed=True, fill=False, linewidth=2, color='y')\n",
    "    ax.axes.add_patch(patch)\n",
    "    plt.text(vertices[0][0], vertices[0][1], text, fontsize=20, va=\"top\")\n",
    "_ = plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
